# Rockchip RK3588å¯åŠ¨OpenAIæ¥å£ä½¿ç”¨è¯´æ˜ ğŸ“„

---

## ç›®å½• ğŸ“‘

1. [ç®€ä»‹](#1-ç®€ä»‹)
2. [ç¯å¢ƒå‡†å¤‡](#2-ç¯å¢ƒå‡†å¤‡)
   - [ç¡¬ä»¶è¦æ±‚](#21-ç¡¬ä»¶è¦æ±‚)
   - [è½¯ä»¶è¦æ±‚](#22-è½¯ä»¶è¦æ±‚)
3. [ç¯å¢ƒå®‰è£…](#3-ç¯å¢ƒå®‰è£…)
   - [å®‰è£…Pythonç¯å¢ƒ](#31-å®‰è£…pythonç¯å¢ƒ)
4. [å…³é”®é…ç½®](#4-å…³é”®é…ç½®)
   - [æ¨¡å‹å­˜æ”¾ä¸é…ç½®](#41-æ¨¡å‹å­˜æ”¾ä¸é…ç½®)
5. [æ¨¡å‹åˆ—è¡¨](#5-æ¨¡å‹åˆ—è¡¨)
6. [å¯åŠ¨OpenAIæœåŠ¡](#6-å¯åŠ¨openaiæœåŠ¡)
7. [æµ‹è¯•](#7-æµ‹è¯•)
   - [ä½¿ç”¨CURLå‘½ä»¤è°ƒè¯•](#71-ä½¿ç”¨curlå‘½ä»¤è°ƒè¯•)
   - [Pythonç¤ºä¾‹](#72-pythonç¤ºä¾‹)
8. [æ›´å¤šèµ„æº](#8-æ›´å¤šèµ„æº)

---

## 1. ç®€ä»‹ ğŸš€

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†å¦‚ä½•åœ¨RK3588å¼€å‘æ¿ä¸Šå¯åŠ¨OpenAIæ¥å£ï¼Œå¹¶ä½¿ç”¨é»˜è®¤çš„RKLLMåº“ï¼ˆç‰ˆæœ¬1.1.4ï¼‰ã€‚é€šè¿‡æœ¬æ–‡æ¡£ï¼Œæ‚¨å°†èƒ½å¤Ÿå¿«é€Ÿé…ç½®ç¯å¢ƒå¹¶å¯åŠ¨OpenAIæ¥å£æœåŠ¡ã€‚

**RKLLM-OpenAI** æ˜¯ä¸€ä¸ªåŸºäºOpenAIæ¥å£ï¼Œæ”¯æŒå¯¹è¯ï¼ˆchatï¼‰ã€è¡¥å…¨ï¼ˆcompletionsï¼‰ç­‰åŠŸèƒ½çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡RKLLMï¼Œæ‚¨å¯ä»¥è½»æ¾é›†æˆå¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›åˆ°æ‚¨çš„åº”ç”¨ä¸­ã€‚

---

## 2. ç¯å¢ƒå‡†å¤‡ ğŸ› ï¸

### 2.1 ç¡¬ä»¶è¦æ±‚
- **RK3588å¼€å‘æ¿**ï¼šç¡®ä¿å¼€å‘æ¿å·²æ­£ç¡®è¿æ¥ç”µæºã€ç½‘ç»œåŠå…¶ä»–å¤–è®¾ã€‚
- **å­˜å‚¨ç©ºé—´**ï¼šè¯·æ ¹æ®ä½¿ç”¨æ¨¡å‹çš„å¤§å°è®¾ç½®å­˜å‚¨ç©ºé—´ï¼Œå»ºè®®è‡³å°‘é¢„ç•™2GBç©ºé—´ã€‚

### 2.2 è½¯ä»¶è¦æ±‚
- **æ“ä½œç³»ç»Ÿ**ï¼šRK3588å¼€å‘æ¿éœ€è¿è¡ŒåŸºäºLinuxçš„ç³»ç»Ÿï¼ˆå¦‚çƒ§å½•é€‚é…çš„Ubuntuæˆ–Debianï¼‰ã€‚
- **Pythonç¯å¢ƒ**ï¼šç¡®ä¿å·²å®‰è£…Python 3.8æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚
- **RKLLMåº“**ï¼šé»˜è®¤ä½¿ç”¨RKLLMåº“ç‰ˆæœ¬1.1.4ã€‚

---

## 3. ç¯å¢ƒå®‰è£… âš™ï¸

### 3.1 å®‰è£…Pythonç¯å¢ƒ
ç¡®ä¿æ‚¨çš„ç³»ç»Ÿå·²å®‰è£…Python 3.10æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ£€æŸ¥Pythonç‰ˆæœ¬ï¼š

```bash
# åœ¨RK3588å¼€å‘æ¿ä¸Šï¼Œé¦–å…ˆæ›´æ–°ç³»ç»Ÿä»¥ç¡®ä¿æ‰€æœ‰è½¯ä»¶åŒ…ä¸ºæœ€æ–°ç‰ˆæœ¬
sudo apt update
sudo apt upgrade -y

# å®‰è£…å¿…è¦çš„Pythonä¾èµ–åŒ…
sudo apt install python3-pip python3-venv -y

# æ£€æŸ¥Pythonç‰ˆæœ¬
python3 --version
```

---

## 4. å…³é”®é…ç½® ğŸ”§

### 4.1 æ¨¡å‹å­˜æ”¾ä¸é…ç½®
- **models**ï¼šå­˜æ”¾RKLLMæ¨¡å‹ã€‚
- **model_configs.py**ï¼šè¯¥æ–‡ä»¶åŒ…å«ç›®å‰æ”¯æŒçš„RKLLMæ¨¡å‹ä¸å…¶å¯¹åº”çš„æ¨ç†å‚æ•°ã€‚

**é…ç½®æ–‡ä»¶ç¤ºä¾‹ï¼š**
```python
"Qwen-2.5-Instruct": {
    "base_config": {
        "st_model_id": "./tokenizers/Qwen2.5-1.5B-Instruct",
        "max_context_len": 8192,
        "max_new_tokens": 8192,
        "top_k": 20,
        "top_p": 0.9,
        "temperature": 0.6,
        "repeat_penalty": 1.1,
        "frequency_penalty": 0.3,
        "system_prompt": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."
    },
    "models": {
        "Qwen2.5-1.5B-Instruct": {"filename": "Qwen2.5-1.5B-Instruct-rk3588-w8a8-opt-0-hybrid-ratio-0.0.rkllm", "parameter_size": "1.5B"},
        "Qwen2.5-3B-Instruct": {"filename": "Qwen2.5-3B-Instruct-rk3588-w8a8-opt-0-hybrid-ratio-0.0.rkllm", "parameter_size": "3B"},
        "Qwen2.5-7B-Instruct": {"filename": "Qwen2.5-7B-Instruct-rk3588-w8a8-opt-0-hybrid-ratio-0.0.rkllm", "parameter_size": "7B"},
    }
}
```

---

## 5. æ¨¡å‹åˆ—è¡¨ ğŸ“‹

ç›®å‰æ”¯æŒå¤šç§æ¨¡å‹ï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼š

- **Llamaç³»åˆ—**ï¼š
  - Llama-3.2-Instruct-3B
  - Llama-3.2-Instruct-1B

- **Qwenç³»åˆ—**ï¼š
  - Qwen2.5-1.5B-Instruct
  - Qwen2.5-3B-Instruct
  - Qwen2.5-7B-Instruct

- **Phiç³»åˆ—**ï¼š
  - Phi-3-mini-4k-instruct
  - Phi-3-mini-128k-instruct

- **å…¶ä»–æ¨¡å‹**ï¼š
  - gemma-2-2b-it-instruct
  - internlm2_5-1_8b-chat
  - deepseek-llm-7b-chat

---

## 6. å¯åŠ¨OpenAIæœåŠ¡ ğŸš€

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨OpenAIæœåŠ¡ï¼š

```bash
python3 rkllm_server_openai.py --host 0.0.0.0 --port 8000
```

---

## 7. æµ‹è¯• ğŸ§ª

### 7.1 ä½¿ç”¨CURLå‘½ä»¤è°ƒè¯•
```bash
curl -L -X POST 'http://<host>:<port>/v1/completions' \
-H 'Content-Type: application/json' \
-H 'Accept: application/json' \
--data-raw '{
  "messages": [
    {
      "content": "You are a helpful assistant",
      "role": "system"
    },
    {
      "content": "Hi",
      "role": "user"
    }
  ],
  "model": "Llama-3.2-Instruct-1B",
  "frequency_penalty": 0,
  "max_tokens": 2048,
  "presence_penalty": 0,
  "response_format": {
    "type": "text"
  }
}'
```

### 7.2 Pythonç¤ºä¾‹
```python
from openai import OpenAI

client = OpenAI(
    base_url="http://<host>:<port>/v1",
    api_key="your-api-key-here",  # ModelScope Token
)

response = client.chat.completions.create(
    model="Llama-3.2-Instruct-1B",
    messages=[
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": "Hello"},
    ],
    max_tokens=1024,
    temperature=0.7,
    stream=False
)

print(response.choices[0].message.content)
```

---

## 8. æ›´å¤šèµ„æº ğŸ“š

- [RKLLMå®˜æ–¹æ–‡æ¡£](https://github.com/airockchip/rknn-llm)
- [OpenAIå®˜æ–¹æ–‡æ¡£](https://platform.openai.com/docs)

é€šè¿‡ä»¥ä¸Šæ–‡æ¡£ï¼Œæ‚¨å¯ä»¥å¿«é€Ÿä¸Šæ‰‹ä½¿ç”¨RKLLMçš„OpenAIæ¥å£ï¼Œé›†æˆå¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›åˆ°æ‚¨çš„åº”ç”¨ä¸­ã€‚